{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base algorithm for features' importance classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "labels = [\"NoEvents\", \"Attack\", \"Natural\"]\n",
    "\n",
    "X = pd.read_csv(\"Data/data%d.csv\"%1) #read file\n",
    "\n",
    "X = X.replace(np.inf, np.finfo(np.float32).max) #replacing 'inf' with its equivalent in float32 datatype\n",
    "\n",
    "#preparing the label converter\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "#assigning the training data and the labels into variables\n",
    "y = le.transform(X['marker'])\n",
    "X = X.drop(columns='marker')\n",
    "\n",
    "features = list(X.columns)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "X=X.values\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "explainer = LimeTabularExplainer(X, training_labels = y, feature_names = features, class_names = labels)\n",
    "\n",
    "\n",
    "Xall = []\n",
    "for i in range(2,16):\n",
    "    Xall.append(pd.read_csv(\"Data/data%d.csv\"%i))\n",
    "\n",
    "Xall = pd.concat(Xall)\n",
    "Xall = Xall.replace(np.inf, np.finfo(np.float32).max) #replacing 'inf' with its equivalent in float32 datatype\n",
    "\n",
    "#assigning the training data and the labels into variables\n",
    "yall = le.transform(Xall['marker'])\n",
    "Xall = Xall.drop(columns='marker')\n",
    "\n",
    "# boole = (yall != clf.predict(Xall))\n",
    "\n",
    "# faulty = boole & (yall == 0)\n",
    "# X_test = Xall[faulty]\n",
    "# y_test = yall[faulty]\n",
    "# lst = []\n",
    "# for idx in range(0, 100):\n",
    "#     exp = explainer.explain_instance(X_test[idx], clf.predict_proba, num_features=128, labels=[0, 1, 2])\n",
    "#     lst.append(exp.as_list(label=0))\n",
    "# lst = np.array(lst)\n",
    "# clst = np.concatenate(lst, axis=0)\n",
    "# dtfr = pd.DataFrame(clst, columns=['feature', 'importance'])\n",
    "# dtfr[\"importance\"] = pd.to_numeric(dtfr[\"importance\"])\n",
    "# dtfr = dtfr.groupby(['feature']).mean()\n",
    "# res = dtfr.sort_values(by=\"importance\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      importance\nfeature                         \nR3:F > 60.00           -0.016330\nR2-PA7:VH <= -101.20   -0.010771\nR4-PA2:VH <= -95.89    -0.008911\nR2-PA6:IH > 81.51      -0.007957\nR1-PA1:VH > 71.28      -0.007917\n...                          ...\nR3-PM6:I <= 318.25      0.005361\nR1-PA:Z > 12.43         0.005471\nR4-PA5:IH > 115.38      0.005963\nR1-PA1:VH <= -97.40     0.006100\nR4-PA2:VH > 117.68      0.013967\n\n[360 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n    <tr>\n      <th>feature</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>R3:F &gt; 60.00</th>\n      <td>-0.016330</td>\n    </tr>\n    <tr>\n      <th>R2-PA7:VH &lt;= -101.20</th>\n      <td>-0.010771</td>\n    </tr>\n    <tr>\n      <th>R4-PA2:VH &lt;= -95.89</th>\n      <td>-0.008911</td>\n    </tr>\n    <tr>\n      <th>R2-PA6:IH &gt; 81.51</th>\n      <td>-0.007957</td>\n    </tr>\n    <tr>\n      <th>R1-PA1:VH &gt; 71.28</th>\n      <td>-0.007917</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>R3-PM6:I &lt;= 318.25</th>\n      <td>0.005361</td>\n    </tr>\n    <tr>\n      <th>R1-PA:Z &gt; 12.43</th>\n      <td>0.005471</td>\n    </tr>\n    <tr>\n      <th>R4-PA5:IH &gt; 115.38</th>\n      <td>0.005963</td>\n    </tr>\n    <tr>\n      <th>R1-PA1:VH &lt;= -97.40</th>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <th>R4-PA2:VH &gt; 117.68</th>\n      <td>0.013967</td>\n    </tr>\n  </tbody>\n</table>\n<p>360 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosen features values modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n    NoEvents       0.72      0.76      0.74     51797\n      Attack       0.27      0.26      0.26     17382\n     Natural       0.19      0.08      0.12      4232\n\n    accuracy                           0.60     73411\n   macro avg       0.39      0.37      0.37     73411\nweighted avg       0.58      0.60      0.59     73411\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(yall, clf.predict(Xall), labels=[0,1,2], target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmod = Xall.copy()\n",
    "\n",
    "def modify(feat, val):\n",
    "    Xmod[feat] =Xmod[feat].apply(lambda x: x + val)\n",
    "\n",
    "\n",
    "modify(\"R4-PA5:IH\", -115.38)\n",
    "modify(\"R3-PM2:V\", 128525.29)\n",
    "modify(\"R2-PM1:V\", 2000)\n",
    "modify(\"R1-PA12:IH\", 32.04)\n",
    "modify(\"R3-PM5:I\", 330.7)\n",
    "\n",
    "modify(\"R3:S\", 0)\n",
    "modify(\"R2-PA7:VH\", 101.20)\n",
    "modify(\"R2-PM1:V\", -1300872.03)\n",
    "modify(\"R3-PA7:VH\", 101.22)\n",
    "modify(\"R3-PA2:VH\", 93.75)\n",
    "\n",
    "\n",
    "\n",
    "modify(\"R2:F\", -60)\n",
    "modify(\"R3:F\", -60)\n",
    "modify(\"R2-PA5:IH\",- 63.30)\n",
    "modify(\"R2-PM7:V\", -130857.40)\n",
    "modify(\"R1-PA1:VH\", -72.28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n    NoEvents       0.71      0.83      0.77     51797\n      Attack       0.26      0.17      0.21     17382\n     Natural       0.10      0.03      0.05      4232\n\n    accuracy                           0.63     73411\n   macro avg       0.36      0.34      0.34     73411\nweighted avg       0.57      0.63      0.59     73411\n\n"
    }
   ],
   "source": [
    "print(classification_report(yall, clf.predict(Xmod), labels=[0,1,2], target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "yall[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(Xall.iloc[15], clf.predict_proba, num_features=128, labels=[0, 1, 2])\n",
    "lst = exp.as_list()\n",
    "lst = np.array(lst)\n",
    "dtfr = pd.DataFrame(lst, columns=['feature', 'importance'])\n",
    "dtfr[\"importance\"] = pd.to_numeric(dtfr[\"importance\"])\n",
    "dtfr = dtfr.groupby(['feature']).mean()\n",
    "res = dtfr.sort_values(by=\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      importance\nfeature                         \nR2-PA8:VH <= 0.00      -0.010952\nrelay4_log <= 0.00     -0.010254\nR3-PA8:VH <= 0.00      -0.007786\nR1-PA8:VH <= 0.00      -0.007464\nR3-PM6:I <= 318.25     -0.007126\n...                          ...\nR2-PM1:V > 130872.03    0.004933\nR2-PM2:V > 130805.80    0.005550\nR4:S <= 0.00            0.005780\nR4-PA2:VH <= -95.89     0.007224\nR1-PA9:VH <= 0.00       0.008653\n\n[128 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n    <tr>\n      <th>feature</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>R2-PA8:VH &lt;= 0.00</th>\n      <td>-0.010952</td>\n    </tr>\n    <tr>\n      <th>relay4_log &lt;= 0.00</th>\n      <td>-0.010254</td>\n    </tr>\n    <tr>\n      <th>R3-PA8:VH &lt;= 0.00</th>\n      <td>-0.007786</td>\n    </tr>\n    <tr>\n      <th>R1-PA8:VH &lt;= 0.00</th>\n      <td>-0.007464</td>\n    </tr>\n    <tr>\n      <th>R3-PM6:I &lt;= 318.25</th>\n      <td>-0.007126</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>R2-PM1:V &gt; 130872.03</th>\n      <td>0.004933</td>\n    </tr>\n    <tr>\n      <th>R2-PM2:V &gt; 130805.80</th>\n      <td>0.005550</td>\n    </tr>\n    <tr>\n      <th>R4:S &lt;= 0.00</th>\n      <td>0.005780</td>\n    </tr>\n    <tr>\n      <th>R4-PA2:VH &lt;= -95.89</th>\n      <td>0.007224</td>\n    </tr>\n    <tr>\n      <th>R1-PA9:VH &lt;= 0.00</th>\n      <td>0.008653</td>\n    </tr>\n  </tbody>\n</table>\n<p>128 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "Xall.iloc[15][\"R1-PA9:VH\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall.at[15, \"R1-PA9:VH\"] = 0\n",
    "Xall.at[15, \"R4-PA2:VH\"] = -95,89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance:\n",
    "    def __init__(self):\n",
    "        self.noevents = None\n",
    "        self.attack = None\n",
    "        self.natural = None\n",
    "\n",
    "    def distance(self, X1, X2):\n",
    "        return np.abs(((X1 - X2).sum()))\n",
    "\n",
    "    def important(self, X):\n",
    "        return X[[\"R4-PA5:IH\", \"R3-PM2:V\", \"R2-PM1:V\", \"R1-PA12:IH\", \"R3-PM5:I\", \"R3:S\", \"R2-PA7:VH\", \"R2-PM1:V\", \"R3-PA7:VH\",\"R3-PA2:VH\", \"R2:F\", \"R3:F\", \"R2-PA5:IH\",\"R2-PM7:V\",\"R1-PA1:VH\"]]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xnew = self.important(X)\n",
    "        self.noevents = Xnew[y == 0].mean(axis=0)\n",
    "        self.attack = Xnew[y == 1].mean(axis=0)\n",
    "        self.natural = Xnew[y == 2].mean(axis=0)     \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xnew = self.important(X)\n",
    "        res = np.c_[np.apply_along_axis(lambda x: self.distance(x, self.noevents), axis=1, arr=Xnew), np.apply_along_axis(lambda x: self.distance(x, self.attack), axis=1, arr=Xnew), np.apply_along_axis(lambda x: self.distance(x, self.natural), axis=1, arr=Xnew)]\n",
    "        return np.c_[Xnew, np.argmin(res, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('dist', <__main__.Distance object at 0x0000020B004B6348>),\n                ('DecisionTree', DecisionTreeClassifier())])"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "labels = [\"NoEvents\", \"Attack\", \"Natural\"]\n",
    "\n",
    "X = pd.read_csv(\"Data/data%d.csv\"%1) #read file\n",
    "\n",
    "X = X.replace(np.inf, np.finfo(np.float32).max) #replacing 'inf' with its equivalent in float32 datatype\n",
    "\n",
    "#preparing the label converter\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "#assigning the training data and the labels into variables\n",
    "y = le.transform(X['marker'])\n",
    "X = X.drop(columns='marker')\n",
    "\n",
    "pipe = Pipeline([('dist', Distance()) , ('DecisionTree', DecisionTreeClassifier())])\n",
    "pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n    NoEvents       0.72      0.80      0.75     51797\n      Attack       0.28      0.22      0.25     17382\n     Natural       0.19      0.08      0.11      4232\n\n    accuracy                           0.62     73411\n   macro avg       0.39      0.37      0.37     73411\nweighted avg       0.58      0.62      0.60     73411\n\n"
    }
   ],
   "source": [
    "Xall = []\n",
    "for i in range(2,16):\n",
    "    Xall.append(pd.read_csv(\"Data/data%d.csv\"%i))\n",
    "\n",
    "Xall = pd.concat(Xall)\n",
    "Xall = Xall.replace(np.inf, np.finfo(np.float32).max) #replacing 'inf' with its equivalent in float32 datatype\n",
    "\n",
    "#assigning the training data and the labels into variables\n",
    "yall = le.transform(Xall['marker'])\n",
    "Xall = Xall.drop(columns='marker')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(yall, pipe.predict(Xall), labels=[0,1,2], target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GaussianHMM(n_components=3)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from hmmlearn.hmm import GaussianHMM\n",
    "clf2 = GaussianHMM(3)\n",
    "clf2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = clf2.get_stationary_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctest = DecisionTreeClassifier(class_weight = {0: coefs[0], 1:coefs[1], 2:coefs[2]})\n",
    "ctest1 = DecisionTreeClassifier(class_weight = {0: coefs[0], 1:coefs[1], 2:coefs[2]}, criterion=\"entropy\")\n",
    "ctest2= DecisionTreeClassifier(class_weight = \"balanced\")\n",
    "ctest3= DecisionTreeClassifier(class_weight = \"balanced\", criterion= \"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n    NoEvents       0.72      0.76      0.74     51797\n      Attack       0.27      0.26      0.26     17382\n     Natural       0.24      0.10      0.14      4232\n\n    accuracy                           0.60     73411\n   macro avg       0.41      0.37      0.38     73411\nweighted avg       0.58      0.60      0.59     73411\n\n              precision    recall  f1-score   support\n\n    NoEvents       0.71      0.78      0.74     51797\n      Attack       0.25      0.23      0.24     17382\n     Natural       0.26      0.09      0.13      4232\n\n    accuracy                           0.61     73411\n   macro avg       0.41      0.36      0.37     73411\nweighted avg       0.58      0.61      0.59     73411\n\n              precision    recall  f1-score   support\n\n    NoEvents       0.71      0.75      0.73     51797\n      Attack       0.27      0.26      0.26     17382\n     Natural       0.16      0.06      0.09      4232\n\n    accuracy                           0.60     73411\n   macro avg       0.38      0.36      0.36     73411\nweighted avg       0.58      0.60      0.59     73411\n\n              precision    recall  f1-score   support\n\n    NoEvents       0.71      0.79      0.75     51797\n      Attack       0.27      0.22      0.24     17382\n     Natural       0.20      0.06      0.10      4232\n\n    accuracy                           0.62     73411\n   macro avg       0.39      0.36      0.36     73411\nweighted avg       0.58      0.62      0.59     73411\n\n              precision    recall  f1-score   support\n\n    NoEvents       0.71      0.78      0.74     51797\n      Attack       0.26      0.24      0.25     17382\n     Natural       0.37      0.08      0.13      4232\n\n    accuracy                           0.61     73411\n   macro avg       0.45      0.37      0.38     73411\nweighted avg       0.58      0.61      0.59     73411\n\n              precision    recall  f1-score   support\n\n    NoEvents       0.71      0.77      0.74     51797\n      Attack       0.26      0.25      0.25     17382\n     Natural       0.34      0.07      0.11      4232\n\n    accuracy                           0.60     73411\n   macro avg       0.44      0.36      0.37     73411\nweighted avg       0.58      0.60      0.59     73411\n\n"
    }
   ],
   "source": [
    "ctest.fit(X,y)\n",
    "ctest1.fit(X,y)\n",
    "ctest2.fit(X,y)\n",
    "ctest3.fit(X,y)\n",
    "\n",
    "clf3= DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf3.fit(X,y)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(yall, clf.predict(Xall), labels=[0,1,2], target_names=labels))\n",
    "print(classification_report(yall, clf3.predict(Xall), labels=[0,1,2], target_names=labels))\n",
    "print(classification_report(yall, ctest.predict(Xall), labels=[0,1,2], target_names=labels))\n",
    "print(classification_report(yall, ctest1.predict(Xall), labels=[0,1,2], target_names=labels))\n",
    "print(classification_report(yall, ctest2.predict(Xall), labels=[0,1,2], target_names=labels))\n",
    "print(classification_report(yall, ctest3.predict(Xall), labels=[0,1,2], target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    " \n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p * coefs[int(class_val)]\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfor index in range(len(dataset[0])-1):\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left)\n",
    "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right)\n",
    "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
    " \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "\troot = get_split(train)\n",
    "\tsplit(root, max_depth, min_size, 1)\n",
    "\treturn root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    " \n",
    "# Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "\ttree = build_tree(train, max_depth, min_size)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(tree, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbis = pd.read_csv(\"Data/data%d.csv\"%1) #read file\n",
    "\n",
    "Xbis = Xbis.replace(np.inf, np.finfo(np.float32).max) #replacing 'inf' with its equivalent in float32 datatype\n",
    "\n",
    "#preparing the label converter\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "#assigning the training data and the labels into variables\n",
    "Xbis[\"marker\"] = le.transform(Xbis['marker'])\n",
    "Xbis = Xbis.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xallv = Xall.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ybis = decision_tree(Xbis, Xallv, 10000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n    NoEvents       0.72      0.77      0.74     51797\n      Attack       0.27      0.25      0.26     17382\n     Natural       0.24      0.08      0.13      4232\n\n    accuracy                           0.61     73411\n   macro avg       0.41      0.37      0.38     73411\nweighted avg       0.58      0.61      0.59     73411\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(yall, ybis, labels=[0,1,2], target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n    NoEvents       0.72      0.77      0.74     51797\n      Attack       0.27      0.25      0.26     17382\n     Natural       0.24      0.08      0.13      4232\n\n    accuracy                           0.61     73411\n   macro avg       0.41      0.37      0.38     73411\nweighted avg       0.58      0.61      0.59     73411\n\n"
    }
   ],
   "source": [
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    "\n",
    "print(classification_report(yall, decision_tree(Xbis, Xallv, 10000, 3), labels=[0,1,2], target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597570240104",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}