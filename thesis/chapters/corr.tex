\chapter{Model enhancement}
After having chosen the appropriate machine learning algorithms and found a way to determine the most important features in the classification problem, this chapter is an attempt to create a model able to enhance the results obtained by the basic classification. Three different approaches are presented. First of all, the most important features values were altered and the effect of this modification is examined. Second, a formula for calculating the distance between different samples is established then a way to use that information. Finally, the Hidden Markov Model were used in order to determine the likehood of prediction of a particular class and this data was used to modify the machine learning algorithm. 

\section{Features values modification} \label{sec:feat}
The first proposed solution considers changing the values of most important features in the dataset after the training process. In other words the training process occurs normally, then the importances are determined and a correction function is created (figure \ref{fig:train}). This correction function will act then on the samples introduced to the model in order to change the features values and obtain better predictions (figure \ref{fig:predict}). 

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[rectangle, draw=black] (main) {Training data};
        \node[rectangle, draw=black] (DecisionTree) [right=of main] {Classifier};
        \node[rectangle, draw=black] (out1) [right=of DecisionTree] {Trained model};
        \node[rectangle, draw=black] (out2) [below=of out1] {Features importances};
        \node[rectangle, draw=black] (feat) [right=of out2] {Features correction model};
   
        \draw[->] (main.east) -- (DecisionTree.west);
        \draw[->] (DecisionTree.east) -- (out1.west);
        \draw[->] (DecisionTree.east) -- (out2.west); 
        \draw[->] (out2.east) -- (feat.west);
    \end{tikzpicture}
    \caption{Training process illustration} \label{fig:train}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[rectangle, draw=black] (main) {Sample};
        \node[rectangle, draw=black] (correl) [right=of main] {Features correction model};
        \node[rectangle, draw=black] (model) [right=of correl] {Trained model};
        \node[rectangle, draw=black](out) [right=of model] {Predicted class};

        \draw[->] (main.east) -- (correl.west);
        \draw[->] (correl.east) -- (model.west);
        \draw[->] (model.east) -- (out.west);
    \end{tikzpicture}
    \caption{Prediction illustration} \label{fig:predict}
\end{figure}

First of all the correction function was defined as the modification function of the feature values for the predicted samples. This modification consists in shifting the feature value so the feature value would not meet the condition to make a false prediction from tables \ref{tab:5best_dt}-\ref{tab:5best_mlp}. This function code in Python is shown in listing \ref{list:modif}.
\begin{python}[caption = {Function used to bulk shift the values of a chosen feature by a given value}, label = list:modif]
def modify(feat, val):
    X[feat] = X[feat].apply(lambda x: x + val)
\end{python}
where \textit{X} is a pandas DataFrame object containing all the samples to predict.

Second, the five most important features are taken from tables \ref{tab:5best_dt}-\ref{tab:5best_mlp} and the values of features from the samples to predict are altered using the previous function. If a feature is duplicated only the value of the first occurrence is taken into account. The code listed in listing \ref{list:corr} shows this operation for Decision Tree classifier.

\begin{python}[caption = {Shifting the values of all the most important features using the condition values found using Lime}, label = list:corr]
modify("R4-PA5:IH", -115.38)
modify("R3-PM2:V", 128525.29)
modify("R2-PM1:V", 2000)
modify("R1-PA12:IH", 32.04)
modify("R3-PM5:I", 330.7)
modify("R3:S", 0)
modify("R2-PA7:VH", 101.20)
modify("R2-PM1:V", -1300872.03)
modify("R3-PA7:VH", 101.22)
modify("R3-PA2:VH", 93.75)    
modify("R2:F", -60)
modify("R3:F", -60)
modify("R2-PA5:IH",- 63.30)
modify("R2-PM7:V", -130857.40) 
\end{python}

The samples modified this way are then used for the predictions. In order to check the success of this method, the classification\_report method from scikit-learn was used. The results before and after modifying the samples are displayed for Decision Tree, Random Forest and MLP classifiers respectively in tables \ref{tab:fm_dt}, \ref{tab:fm_rf} and \ref{tab:fm_mlp}. Those tables are composed of 4 columns: precision, recall, f-measure and support. The first three correspond to the metrics discussed in section \ref{sec:metrics}, while support indicates the number of samples. The tables contain also 6 rows: NoEvents, Attack, Natural, accuracy, macro avg, weighted avg. The first three indicate the classes, accuracy is just the accuracy metric and the last two rows indicate the average values for each column, in particular the macro average and the weighted average.

\begin{table}[H]
    \centering
    \footnotesize
    \caption{Features modification results for Decision Tree classifier} \label{tab:fm_dt}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before features modification} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
                NoEvents  &   $  0.72 $  &  $ 0.76 $  &  $ 0.74 $  & $ 51797 $\\
                  Attack   &  $  0.27 $   & $ 0.26 $  &  $ 0.26 $  & $ 17382 $\\
                 Natural   &  $  0.19 $   & $ 0.08 $  &  $ 0.12 $  & $  4232 $\\
                accuracy   &            &          &  $0.60$  &   $73411$ \\
               macro avg   &  $  0.39 $   & $ 0.37 $  &  $ 0.37 $  & $ 73411 $\\
            weighted avg   &  $  0.58 $  &  $ 0.60 $  &  $ 0.59 $ &  $ 73411 $\\\bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After features modification}
        \begin{tabular}{rcccc}\toprule
            &precision   & recall & f-measure &  support  \\\midrule
    
            NoEvents   &    0.71   &   0.83  &    0.77   &  51797 \\
              Attack    &   0.26   &   0.17  &    0.21   &  17382 \\
             Natural   &    0.10   &   0.03   &   0.05  &    4232 \\
        
            accuracy    &          &          &   0.63   &  73411 \\
           macro avg    &   0.36   &   0.34   &   0.34   &  73411 \\
        weighted avg     &  0.57   &   0.63   &   0.59   &  73411    \\     \bottomrule   
        \end{tabular}
    \end{subtable}
\end{table}

\begin{table}[H]
    \centering
    \footnotesize
    \caption{Features modification results for Random Forest classifier} \label{tab:fm_rf}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before features modification} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
            NoEvents   &    0.72  &    0.87 &     0.78 &  51797 \\
            Attack     &  0.29    &  0.16   &   0.21   &  17382 \\
           Natural     &  0.25    &  0.05   &   0.08   &   4232 \\
          accuracy     &          &         &   0.65   &  73411 \\
         macro avg     &  0.42    &  0.36   &   0.36   &  73411 \\
      weighted avg     &  0.59    &  0.65   &   0.61   &  73411 \\ \bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After features modification}
        \begin{tabular}{rcccc}\toprule
            &precision   & recall & f-measure &  support  \\\midrule
            NoEvents  &     0.71 &     0.89 &     0.79 &  51797\\
            Attack    &   0.28   &   0.13   &   0.18   &  17382\\
           Natural    &   0.26   &   0.05   &   0.08   &   4232\\
          accuracy    &          &          &   0.66   &  73411\\
         macro avg    &   0.42   &   0.36   &   0.35   &  73411\\
      weighted avg    &   0.58   &   0.66   &   0.61   &  73411\\     \bottomrule   
        \end{tabular}
    \end{subtable}
\end{table}

\begin{table}[H]
    \centering
    \footnotesize
    \caption{Features modification results for MLP classifier} \label{tab:fm_mlp}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before features modification} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
            NoEvents   &    0.71 &     0.99 &     0.83&    51797\\
            Attack     &  0.57   &   0.05   &   0.09  &   17382\\
           Natural     &  0.00   &   0.00   &   0.00  &    4232\\    
          accuracy     &         &          &   0.71  &   73411\\
         macro avg     &  0.43   &   0.35   &   0.30  &   73411\\
      weighted avg     &  0.64   &   0.71   &   0.60  &   73411\\ \bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After features modification}
        \begin{tabular}{rcccc}\toprule
            &precision   & recall & f-measure &  support  \\\midrule
            NoEvents   &    0.74  &    0.07 &     0.12 &  51797\\
            Attack     &  0.24    &  0.93   &   0.38   &  17382\\
           Natural     &  0.00    &  0.00   &   0.00   &   4232\\   
          accuracy     &          &         &   0.27   &  73411\\
         macro avg     &  0.33    &  0.33   &   0.17   &  73411\\
      weighted avg     &  0.58    &  0.27   &   0.17   &  73411\\    \bottomrule   
        \end{tabular}
    \end{subtable}
\end{table}

The accuracy of the model has slightly increased for both Decision Tree and Random Forest classifiers, and significally decreased for MLP classifier. Moreover, for Decision Tree classifier, a significant decrease of other metrics for Natural class is observed, for Random Forest some slight increases and decreases of metrics are observed, depending on the class. The most important difference is observed for MLP classifier, where the recall value for NoEvents class passed from $0.99$ to $0.07$, and in parallel, for Attack class from $0.05$ to $0.93$.

It may be concluded that this method succeeded to enhance the results for Decision Tree and Random Forest classifiers, but failed with MLP. It is clear that the model, after modifying the features, tends to predict NoEvents class for the first two classifiers, but Attack class for MLP.  

\section{Distance between features}
The second proposed solution considers using a transformation routine, which, reduces the number of features to the 15 most important from tables \ref{tab:5best_dt}-\ref{tab:5best_mlp} and adds another feature that represents the closest class to the treated sample. The prediction steps were illustrated in figure \ref{fig:distill}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[rectangle, draw=black](sample) {Sample};
        \node[rectangle, draw=black](trans) [right=of sample] {Transformation};
        \node[rectangle, draw=black](class) [right=of trans] {Classifier};
        \node[rectangle, draw=black](out) [right=of class] {Predicted class};

        \draw[->] (sample.east) -- (trans.west);
        \draw[->] (trans.east) -- (class.west);
        \draw[->] (class.east) -- (out.west);
    \end{tikzpicture}
    \caption{Distance algorithm illustration}
    \label{fig:distill}
\end{figure}

The transformation routine on other hand takes the form of a python class and is composed of 4 methods:
\begin{enumerate}
    \item \textbf{distance(X1, X2)}: returns the distance between two samples X1 and X2. It is calculated as the sum of differences between features,
    \item \textbf{important(X)}: returns the samples with only 15 most important features. The input must be a pandas DataFrame or Series. The choice of features to keep is made by hand directly in the method, without the possibility to change them afterwards,
    \item \textbf{fit(X,y)}: determines the reference class sample for each class by calculating the mean value of each feature among all samples corresponding to the treated class. It is called only during data fitting to the classifier,
    \item \textbf{transform(X)}: determines the class with the smallest distance to the sample, based on reference samples determined by fit(X,y) method. The obtained values are then added as a new feature to the samples X and returned afterwards by the method.
\end{enumerate}

This routine has been coupled with the 3 chosen classifiers using \textbf{pipeline} class in scikit-learn, which construct acts like a classifier (it has fit and predict methods). 

The success rate of this method was verified, once more, using classification\_report method from scikit-learn. The result and after using the described method are shown for Decision Tree, Random Forest and MLP classifiers respectively in tables \ref{tab:dist_dt}, \ref{tab:dist_rf} and \ref{tab:dist_mlp}. They have the same composition as the tables from section \ref{sec:feat}.

\begin{table}[H]
    \centering \footnotesize
    \caption{Distance routine results for Decision Tree classifier}  \label{tab:dist_dt}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before using the distance routine} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
                NoEvents  &   $  0.72 $  &  $ 0.76 $  &  $ 0.74 $  & $ 51797 $\\
                  Attack   &  $  0.27 $   & $ 0.26 $  &  $ 0.26 $  & $ 17382 $\\
                 Natural   &  $  0.19 $   & $ 0.08 $  &  $ 0.12 $  & $  4232 $\\
                accuracy   &            &          &  $0.60$  &   $73411$ \\
               macro avg   &  $  0.39 $   & $ 0.37 $  &  $ 0.37 $  & $ 73411 $\\
            weighted avg   &  $  0.58 $  &  $ 0.60 $  &  $ 0.59 $ &  $ 73411 $\\\bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After using the distance routine} 
        \begin{tabular}{rcccc}\toprule
         &   precision    &recall & f-measure &  support  \\\midrule
    
            NoEvents    &   0.72   &   0.79   &   0.75  &   51797 \\
              Attack    &   0.28   &   0.23   &   0.25  &   17382 \\
             Natural   &    0.19   &   0.08   &   0.11  &    4232 \\
        
            accuracy    &           &         &   0.62   &  73411 \\
           macro avg    &   0.40    &  0.37   &   0.37  &   73411 \\
        weighted avg   &   0.58   &   0.62   &   0.60   &  73411   \\  \bottomrule
        \end{tabular}
    \end{subtable}
\end{table}

\begin{table}[H]
    \centering \footnotesize
    \caption{Distance routine results for Random Forest classifier}  \label{tab:dist_rf}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before using the distance routine} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
            NoEvents   &    0.72  &    0.87 &     0.78 &  51797 \\
            Attack     &  0.29    &  0.16   &   0.21   &  17382 \\
           Natural     &  0.25    &  0.05   &   0.08   &   4232 \\
          accuracy     &          &         &   0.65   &  73411 \\
         macro avg     &  0.42    &  0.36   &   0.36   &  73411 \\
      weighted avg     &  0.59    &  0.65   &   0.61   &  73411 \\ \bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After using the distance routine} 
        \begin{tabular}{rcccc}\toprule
         &   precision    &recall & f-measure &  support  \\\midrule
    
        NoEvents   &    0.71 &     0.83 &     0.77 &   51797\\
         Attack    &   0.27  &    0.19  &    0.22  &   17382\\
        Natural    &   0.18  &    0.05  &    0.07  &    4232\\
       accuracy    &         &          &    0.63  &   73411\\
      macro avg    &   0.39  &    0.36  &    0.36  &   73411\\
   weighted avg    &   0.58  &    0.63  &    0.60  &   73411\\  \bottomrule
        \end{tabular}
    \end{subtable}
\end{table}

\begin{table}[H]
    \centering \footnotesize
    \caption{Distance routine results for MLP classifier}  \label{tab:dist_mlp}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before using the distance routine} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
            NoEvents   &    0.71 &     0.99 &     0.83&    51797\\
            Attack     &  0.57   &   0.05   &   0.09  &   17382\\
           Natural     &  0.00   &   0.00   &   0.00  &    4232\\    
          accuracy     &         &          &   0.71  &   73411\\
         macro avg     &  0.43   &   0.35   &   0.30  &   73411\\
      weighted avg     &  0.64   &   0.71   &   0.60  &   73411\\ \bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After using the distance routine} 
        \begin{tabular}{rcccc}\toprule
         &   precision    &recall & f-measure &  support  \\\midrule
    
         NoEvents  &     0.70 &     0.98 &     0.82 &  51797\\
         Attack    &   0.18   &   0.01   &   0.02   &  17382\\
        Natural    &   0.00   &   0.00   &   0.00   &   4232\\
       accuracy    &          &          &   0.69   &  73411\\
      macro avg    &   0.29   &   0.33   &   0.28   &  73411\\
   weighted avg    &   0.54   &   0.69   &   0.58   &  73411\\  \bottomrule
        \end{tabular}
    \end{subtable}
\end{table}

The tables \ref{tab:dist_dt}-\ref{tab:dist_mlp} show an increase of accuracy by 0.02 in the Decision Tree classifier case, but a decrease by also 0.02 in the Random Forest and MLP classifiers case. For Decision Tree classifier, the other metrics shows a slight increase, except recall and f-measure for Attack class, however this decrease was not significant. On other hand for the two remaining classifiers a decrease in values of almost all the metrics is observed.

It can be concluded that this method does work only using Decision Tree classifier and gives results better than the features' modification method. For Random Forest and MLP classifiers this method fails to enhance the predictions.

\section{Hidden Markov Model}
The last proposed solution consists in determining the probabilities of occurrence of different classes using so called Hidden Markov Model. The obtained probabilities are then used to modify the split condition in the decision tree classifier. 

The Hidden Markov Model is a probabilistic model for generating observable data in a random way by a sequence of internal hidden states, which can not be observed directly. The transitions between those hidden states have the form of a Markov chain \cite{noauthor_tutorial_nodate}. The Markov chain, on other hand, is a chain of interconnected states, where each state's probability depends only of the probability of predecessing state, without taking into consideration what happened before \cite{amit_introduction_2019}.

In the studied example, the observable data are the features values available in the samples and the states are represented by the 3 classes NoEvents, Attack and Normal. The Hidden Markov Model was used to determine the stationary distribution of the states (invariable in time probability distribution in the Markov chain).

It exists a Python package that implements the Hidden Markov Models and it is compatible with scikit-learn, it's name is \textbf{hmmlearn} \cite{noauthor_hmmlearn_2020}. It can be used just like any other scikit-learn classifiers, especially that it offers methods like fit(), predict(), predict\_proba(), etc...

Using \textbf{hmmlearn} the stationary distribution was determined in 4 lines of code as shown in listing \ref{list:hmm}.
\begin{python}[caption = {Determination of the stationary distribution of the classes using hmmlearn}, label = list:hmm]
from hmmlearn.hmm import GaussianHMM 
clf2 = GaussianHMM(n_components = 3) # 3 states
clf2.fit(X)
coefs = clf2.get_stationary_distribution()
\end{python}
The Gaussian HMM was used because the observations (features) are continuous. It was initialized by a number of components equal to 3 because of 3 states - NoEvents, Attack and Natural.

The stationary distribution obtained this way was then used to set the class\_weight parameter in DecisionTreeClassifier in scikit-learn, as shown in the listing \ref{list:dt}.

\begin{python}[caption = {Definition of the Decision Tree Classifier taking into consideration the stationary distribution}, label = list:dt]
ctest = DecisionTreeClassifier(class_weight = {0: coefs[0], 1:coefs[1], 2:coefs[2]})  
\end{python}

For Random Forest classifier the same parameter exists and was used to produce results. However, for MLP classifier a similar parameter does not exist and using the determined coefficients would require deeper modifications of the neural network.

The results for Decision Tree and Random Forest classifiers are presented in tables \ref{tab:hmm_dt} and \ref{tab:hmm_rf}. They have the same composition as the tables from section \ref{sec:feat}.

\begin{table}[H]
    \centering \footnotesize
    \caption{Hidden Markov Model manipulation results for Decision Tree classifier}  \label{tab:hmm_dt}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before using HMM} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
                NoEvents  &   $  0.72 $  &  $ 0.76 $  &  $ 0.74 $  & $ 51797 $\\
                  Attack   &  $  0.27 $   & $ 0.26 $  &  $ 0.26 $  & $ 17382 $\\
                 Natural   &  $  0.19 $   & $ 0.08 $  &  $ 0.12 $  & $  4232 $\\
                accuracy   &            &          &  $0.60$  &   $73411$ \\
               macro avg   &  $  0.39 $   & $ 0.37 $  &  $ 0.37 $  & $ 73411 $\\
            weighted avg   &  $  0.58 $  &  $ 0.60 $  &  $ 0.59 $ &  $ 73411 $\\\bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After using HMM}
        \begin{tabular}{rcccc}\toprule
        & precision &   recall & f-measure  & support \\\midrule

            NoEvents    &   0.71    &  0.75  &    0.73     &51797\\
            Attack     &  0.27    &  0.26  &    0.26 &   17382\\
            Natural    &   0.16  &    0.06    &  0.09  &    4232\\
        
            accuracy     &          &         &   0.60   &  73411\\
        macro avg     &  0.38    &  0.36   &   0.36  &   73411\\
        weighted avg    &   0.58    &  0.60   &   0.59  &   73411   \\ \bottomrule   
        \end{tabular}
    \end{subtable}
\end{table}

\begin{table}[H]
    \centering \footnotesize
    \caption{Hidden Markov Model manipulation results for Random Forest classifier}  \label{tab:hmm_rf}
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{Before using HMM} 
        \begin{tabular}{rcccc}\toprule
            & precision    &recall & f-measure  & support \\\midrule
            NoEvents   &    0.72  &    0.87 &     0.78 &  51797 \\
            Attack     &  0.29    &  0.16   &   0.21   &  17382 \\
           Natural     &  0.25    &  0.05   &   0.08   &   4232 \\
          accuracy     &          &         &   0.65   &  73411 \\
         macro avg     &  0.42    &  0.36   &   0.36   &  73411 \\
      weighted avg     &  0.59    &  0.65   &   0.61   &  73411 \\ \bottomrule
        \end{tabular}
    \end{subtable}%
    \begin{subtable}[t]{.5\linewidth}
        \centering
        \caption{After using HMM}
        \begin{tabular}{rcccc}\toprule
        & precision &   recall & f-measure  & support \\\midrule
        NoEvents   &    0.72 &     0.85  &    0.78 &  51797\\
        Attack     &  0.30   &   0.19    &  0.24   &  17382\\
       Natural     &  0.25   &   0.05    &  0.09   &   4232\\
      accuracy     &         &           &  0.65   &  73411\\
     macro avg     &  0.42   &   0.37    &  0.37   &  73411\\
  weighted avg     &  0.59   &   0.65    &  0.61   &  73411\\ \bottomrule   
        \end{tabular}
    \end{subtable}
\end{table}

The tables \ref{tab:hmm_dt} and \ref{tab:hmm_rf} show that the accuracy did not change after using the probabilities obtained from Hidden Markov Model. Looking more in details, it can be observed a general small decrease of the majority of the other metrics in the Decision Tree classifier case, but, in Random Forest classifier case, a slight increase in metrics value in relation to the Attack class and a small decrease of recall for NoEvents.

It may be concluded that this method does not work with the given dataset, however the results obtained by the Random Forest classifier are better than those provided by Decision Tree classifier. The possible cause of the failure of this method may be the fact that the samples in the dataset does not include any information about their order in time. If the dataset contained information about the time of the event, the results, probably, would be more significant. \\

In this chapter, three methods for results' enhancement were presented. The results vary depending on the used machine learning technique. For Decision Tree classifier, the best method for enhancing the results was the second, using the distance between features. For Random Forest classifier, the best method was the first, modifying the values of features. Finally, for MLP classifier, both two tested methods failed, but the second did not decrease the metrics that drastically as the first method.