
@misc{adhikari_power_2014,
  title = {Power {{System Attack Datasets}}},
  author = {Adhikari, Uttam and Pan, Shengyi and Morris, Tommy},
  year = {2014},
  month = apr,
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\E5NIBS6P\\PowerSystem_Dataset_README.pdf},
  language = {English}
}

@misc{amit_introduction_2019,
  title = {Introduction to {{Hidden Markov Models}}},
  author = {Amit, Tomer},
  year = {2019},
  month = oct,
  abstract = {We give a short introduction to Markov Chains and HMM.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\MSL5D7D8\\introduction-to-hidden-markov-models-cd2c93e6b781.html},
  howpublished = {https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781},
  journal = {Medium},
  language = {en}
}

@misc{ando_saabas_treeinterpreter_nodate,
  title = {{{TreeInterpreter}}},
  author = {{Ando Saabas}},
  abstract = {Package for interpreting scikit-learn's decision tree and random forest predictions. Allows decomposing each prediction into bias and feature contribution components as described in http://blog.datadive.net/interpreting-random-forests/.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\9MLCL9VA\\interpreting-random-forests.html},
  howpublished = {https://github.com/andosa/treeinterpreter}
}

@software{bengfort_yellowbrick_2018,
  title = {Yellowbrick},
  author = {Bengfort, Benjamin and Bilbro, Rebecca and Danielsen, Nathan and Gray, Larry and McIntyre, Kristen and Roman, Prema and Poh, Zijie and others},
  year = {2018-11-14, 2018},
  doi = {10.5281/zenodo.1206264},
  abstract = {Yellowbrick is an open source, pure Python project that extends the Scikit-Learn API with visual analysis and diagnostic tools. The Yellowbrick API also wraps Matplotlib to create publication-ready figures and interactive data explorations while still allowing developers fine-grain control of figures. For users, Yellowbrick can help evaluate the performance, stability, and predictive value of machine learning models, and assist in diagnosing problems throughout the machine learning workflow.},
  copyright = {Apache License 2.0},
  version = {0.9.1}
}

@inproceedings{borges_hink_machine_2014-1,
  title = {Machine Learning for Power System Disturbance and Cyber-Attack Discrimination},
  booktitle = {2014 7th {{International Symposium}} on {{Resilient Control Systems}} ({{ISRCS}})},
  author = {Borges Hink, Raymond C. and Beaver, Justin M. and Buckner, Mark A. and Morris, Tommy and Adhikari, Uttam and Pan, Shengyi},
  year = {2014},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  doi = {10.1109/ISRCS.2014.6900095},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\NMTMU4PM\\Borges Hink et al. - 2014 - Machine learning for power system disturbance and .pdf},
  isbn = {978-1-4799-4187-2 978-1-4799-7221-0}
}

@article{breiman_random_2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  volume = {45},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\DEJC79TM\\Breiman - 2001 - Random Forests.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@misc{brown_pypsapypsa_2020,
  title = {{{PyPSA}}/{{PyPSA}}: {{PyPSA Version}} 0.17.1},
  shorttitle = {{{PyPSA}}/{{PyPSA}}},
  author = {Brown, Tom and H{\"o}rsch, Jonas and FabianHofmann and Neumann, Fabian and Smith, Russell and Chloe and Schlachtberger and Jdedecca and Stianchris and Plas, Fons Van Der and Hailiang Liu and Maria, Martha and Yu, Greta and Mariia Bilousova and Lavoie, Michel and Duong, Minh Ha and Nmartensen and Belotti, Pietro and Dcradu and Jankaeh},
  year = {2020},
  month = jul,
  doi = {10.5281/ZENODO.3946412},
  abstract = {Please see the release notes. Older versions of PyPSA (Versions 0.17.0 and below) can be found under another zenodo DOI.},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@inproceedings{ferragut_real-time_2017,
  title = {Real-{{Time Cyber}}-{{Physical False Data Attack Detection}} in {{Smart Grids Using Neural Networks}}},
  booktitle = {2017 {{International Conference}} on {{Computational Science}} and {{Computational Intelligence}} ({{CSCI}})},
  author = {Ferragut, Erik M. and Laska, Jason and Olama, Mohammed M. and Ozmen, Ozgur},
  year = {2017},
  month = dec,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CSCI.2017.1},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\UMRB92KR\\Ferragut et al. - 2017 - Real-Time Cyber-Physical False Data Attack Detecti.pdf},
  isbn = {978-1-5386-2652-8},
  keywords = {anomaly_cps}
}

@inproceedings{jiang_learning_2015,
  title = {Learning {{Spatial Decision Trees}} for {{Land Cover Mapping}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Data Mining Workshop}} ({{ICDMW}})},
  author = {Jiang, Zhe},
  year = {2015},
  month = nov,
  pages = {1522--1529},
  publisher = {{IEEE}},
  address = {{Atlantic City, NJ, USA}},
  doi = {10.1109/ICDMW.2015.91},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\4WRDDB45\\Jiang - 2015 - Learning Spatial Decision Trees for Land Cover Map.pdf},
  isbn = {978-1-4673-8493-3}
}

@article{jiang_spatial_nodate,
  title = {Spatial {{Decision Tree}}: {{A Novel Approach}} to {{Land}}-{{Cover Classification}}},
  author = {Jiang, Zhe and Shekhar, Shashi and Zhou, Xun and Knight, Joseph and Corcoran, Jennifer},
  pages = {25},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\K7BQ4LBJ\\Jiang et al. - Spatial Decision Tree A Novel Approach to Land-Co.pdf},
  language = {en}
}

@article{jl_kirtley_jr_introduction_nodate,
  title = {Introduction {{To Symmetrical Components}}},
  author = {{J.L. Kirtley Jr.}},
  volume = {Class Notes Chapter 4},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\QEBF6T4D\\Kirtley - 6.061 Class Notes, Chapter 4 Introduction To Symm.pdf},
  journal = {6.061 Introduction to Power Systems},
  language = {en}
}

@article{karpatne_physics-guided_2018,
  title = {Physics-Guided {{Neural Networks}} ({{PGNN}}): {{An Application}} in {{Lake Temperature Modeling}}},
  shorttitle = {Physics-Guided {{Neural Networks}} ({{PGNN}})},
  author = {Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
  year = {2018},
  month = feb,
  abstract = {This paper introduces a novel framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed as physics-guided neural network (PGNN), leverages the output of physics-based model simulations along with observational features to generate predictions using a neural network architecture. Further, this paper presents a novel framework for using physics-based loss functions in the learning objective of neural networks, to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results.},
  archivePrefix = {arXiv},
  eprint = {1710.11431},
  eprinttype = {arxiv},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\SGIR5PK8\\Karpatne et al. - 2018 - Physics-guided Neural Networks (PGNN) An Applicat.pdf;C\:\\Users\\bhadr\\Zotero\\storage\\MMDNYMC8\\1710.html},
  journal = {arXiv:1710.11431 [physics, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  primaryClass = {physics, stat}
}

@article{karpatne_theory-guided_2017,
  title = {Theory-{{Guided Data Science}}: {{A New Paradigm}} for {{Scientific Discovery}} from {{Data}}},
  shorttitle = {Theory-{{Guided Data Science}}},
  author = {Karpatne, Anuj and Atluri, Gowtham and Faghmous, James H. and Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin},
  year = {2017},
  month = oct,
  volume = {29},
  pages = {2318--2331},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2017.2720168},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\KK7WFSAA\\Karpatne et al. - 2017 - Theory-Guided Data Science A New Paradigm for Sci.pdf},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  number = {10}
}

@inproceedings{lime,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining, San Francisco, {{CA}}, {{USA}}, August 13-17, 2016},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  pages = {1135--1144}
}

@article{lundberg_local_2020-2,
  title = {From Local Explanations to Global Understanding with Explainable {{AI}} for Trees},
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  year = {2020},
  month = jan,
  volume = {2},
  pages = {56--67},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0138-9},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\4JEBXRLJ\\Lundberg et al. - 2020 - From local explanations to global understanding wi.pdf},
  journal = {Nature Machine Intelligence},
  language = {en},
  number = {1}
}

@misc{mikhail_korobov_eli5_nodate,
  title = {{{ELI5}}},
  author = {{Mikhail Korobov} and {Konstantin Lopuhin}},
  abstract = {ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions.},
  howpublished = {https://github.com/TeamHG-Memex/eli5}
}

@misc{morris_industrial_nodate,
  title = {Industrial {{Control System}} ({{ICS}}) {{Cyber Attack Datasets}} - {{Tommy Morris}}},
  author = {Morris, Tommy},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\HRLL5AIM\\ics-data-sets.html},
  howpublished = {https://sites.google.com/a/uah.edu/tommy-morris-uah/ics-data-sets},
  language = {English}
}

@misc{noauthor_117_nodate,
  title = {1.17. {{Neural}} Network Models (Supervised) \textemdash{} Scikit-Learn 0.23.1 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\LYFLXPE2\\neural_networks_supervised.html},
  howpublished = {https://scikit-learn.org/stable/modules/neural\_networks\_supervised.html\#mathematical-formulation}
}

@misc{noauthor_beginners_nodate,
  title = {A {{Beginner}}'s {{Guide}} to {{Python Machine Learning}} and {{Data Science Frameworks}}},
  abstract = {A guide to Python tools for artificial intelligence, machine learning and data science.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\HICFM8CV\\python-ai.html},
  howpublished = {http://pathmind.com/wiki/python-ai},
  journal = {Pathmind},
  language = {en}
}

@misc{noauthor_hmmlearn_2020,
  title = {Hmmlearn},
  year = {2020},
  month = aug,
  abstract = {Hidden Markov Models in Python, with scikit-learn like API},
  copyright = {BSD-3-Clause License         ,                 BSD-3-Clause License},
  howpublished = {https://github.com/hmmlearn/hmmlearn}
}

@misc{noauthor_keras_nodate,
  title = {Keras: The {{Python}} Deep Learning {{API}}},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\DVPCCXZ2\\keras.io.html},
  howpublished = {https://keras.io/}
}

@misc{noauthor_libsvm_nodate,
  title = {{{LibSVM}}},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\FAAIA68U\\LibSVM.html},
  howpublished = {https://javadoc.scijava.org/Weka/weka/classifiers/functions/LibSVM.html}
}

@misc{noauthor_naivebayes_nodate,
  title = {{{NaiveBayes}}},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\2WHPWNI7\\NaiveBayes.html},
  howpublished = {https://weka.sourceforge.io/doc.dev/weka/classifiers/bayes/NaiveBayes.html}
}

@misc{noauthor_neural_nodate,
  title = {Neural Network Models (Supervised)},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\DXBKSKIB\\neural_networks_supervised.html},
  howpublished = {https://scikit-learn.org/stable/modules/neural\_networks\_supervised.html\#mathematical-formulation},
  journal = {scikit-learn 0.23.1 documentation}
}

@misc{noauthor_pandapower_nodate,
  title = {Pandapower},
  abstract = {An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\IM5DNDLW\\www.pandapower.org.html},
  howpublished = {http://www.pandapower.org/},
  journal = {pandapower},
  language = {en}
}

@misc{noauthor_pytorch_nodate,
  title = {{{PyTorch}}},
  abstract = {An open source deep learning platform that provides a seamless path from research prototyping to production deployment.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\2Q35FS5Y\\pytorch.org.html},
  howpublished = {https://www.pytorch.org},
  language = {en}
}

@misc{noauthor_randomforest_nodate,
  title = {{{RandomForest}}},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\QFCEECTQ\\RandomForest.html},
  howpublished = {https://weka.sourceforge.io/doc.dev/weka/classifiers/trees/RandomForest.html}
}

@misc{noauthor_sklearnensemblerandomforestclassifier_nodate,
  title = {Sklearn.Ensemble.{{RandomForestClassifier}} \textemdash{} Scikit-Learn 0.23.1 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\K9DQU9ZF\\sklearn.ensemble.RandomForestClassifier.html},
  howpublished = {https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}
}

@misc{noauthor_sklearnnaive_bayesgaussiannb_nodate,
  title = {Sklearn.Naive\_bayes.{{GaussianNB}} \textemdash{} Scikit-Learn 0.23.1 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\A5JWQDV8\\sklearn.naive_bayes.GaussianNB.html},
  howpublished = {https://scikit-learn.org/stable/modules/generated/sklearn.naive\_bayes.GaussianNB.html}
}

@misc{noauthor_sklearnneural_networkmlpclassifier_nodate,
  title = {Sklearn.Neural\_network.{{MLPClassifier}} \textemdash{} Scikit-Learn 0.23.1 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\CN2JRJYG\\sklearn.neural_network.MLPClassifier.html},
  howpublished = {https://scikit-learn.org/stable/modules/generated/sklearn.neural\_network.MLPClassifier.html\#sklearn.neural\_network.MLPClassifier}
}

@misc{noauthor_sklearnsvmsvc_nodate,
  title = {Sklearn.Svm.{{SVC}} \textemdash{} Scikit-Learn 0.23.1 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\8BKR2RWC\\sklearn.svm.SVC.html},
  howpublished = {https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}
}

@misc{noauthor_support_2014,
  title = {Support {{Vector Machines}}, {{Clearly Explained}}!!!},
  year = {2014},
  month = jan,
  abstract = {Support Vector Machines are one of the most mysterious methods in Machine Learning. This StatQuest sweeps away the mystery to let know how they work.

NOTE: This StatQuest assumes you already know about...
The bias/variance tradeoff: https://youtu.be/EuBBz3bI-aA
Cross Validation: https://youtu.be/fSytzGwwBVw

ALSO NOTE: This StatQuest is based on description of Support Vector Machines, and associated concepts, found on pages 337 to 354 of the Introduction to Statistical Learning in R: http://faculty.marshall.usc.edu/garet...

I also found this blogpost helpful for understanding the Kernel Trick: https://blog.statsbot.co/support-vect...

For a complete index of all the StatQuest videos, check out:
https://statquest.org/video-index/

If you'd like to support StatQuest, please consider...
Patreon: https://www.patreon.com/statquest
...or...
YouTube Membership: https://www.youtube.com/channel/UCtYL...

...a cool StatQuest t-shirt or sweatshirt (USA/Europe): https://teespring.com/stores/statquest
(everywhere):
https://www.redbubble.com/people/star...

...buying one or two of my songs (or go large and get a whole album!)
https://joshuastarmer.bandcamp.com/

...or just donating to StatQuest!
https://www.paypal.me/statquest

Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter:
https://twitter.com/joshuastarmer

0:00 Awesome song and introduction
0:40 Basic concepts and Maximal Margin Classifiers
4:35 Soft Margins (allowing misclassifications)
6:46 Soft Margin and Support Vector Classifiers
12:23 Intuition behind Support Vector Machines
15:25 The polynomial kernel function
17:30 The radial basis function (RBF) kernel
18:32 The kernel trick
19:31 Summary of concepts

\#statquest \#SVM}
}

@article{noauthor_symmetrical_2019,
  title = {Symmetrical Components},
  year = {2019},
  month = nov,
  abstract = {In electrical engineering, the method of symmetrical components simplifies analysis of unbalanced three-phase power systems under both normal and abnormal conditions. The basic idea is that an asymmetrical set of N phasors can be expressed as a linear combination of N symmetrical sets of phasors by means of a complex linear transformation.In the most common case of three-phase systems, the resulting "symmetrical" components are referred to as direct (or positive), inverse (or negative) and zero (or homopolar). The analysis of power system is much simpler in the domain of symmetrical components, because the resulting equations are mutually linearly independent if the circuit itself is balanced.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\SBMN4ICT\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{noauthor_three-phase_2020,
  title = {Three-Phase Electric Power},
  year = {2020},
  month = may,
  abstract = {Three-phase electric power is a common method of alternating current electric power generation, transmission, and distribution. It is a type of polyphase system and is the most common method used by electrical grids worldwide to transfer power. It is also used to power large motors and other heavy loads.
A three-wire three-phase circuit is usually more economical than an equivalent two-wire single-phase circuit at the same line to ground voltage because it uses less conductor material to transmit a given amount of electrical power.
Polyphase power systems were independently invented by Galileo Ferraris, Mikhail Dolivo-Dobrovolsky, Jonas Wenstr\"om, John Hopkinson and Nikola Tesla in the late 1880s.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\PK6M4VQH\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{noauthor_tutorial_nodate,
  title = {Tutorial \textemdash{} Hmmlearn 0.2.3.Post13+ge872126 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\W7T2QKTJ\\tutorial.html},
  howpublished = {https://hmmlearn.readthedocs.io/en/latest/tutorial.html\#building-hmm-and-generating-samples}
}

@misc{noauthor_who_nodate,
  title = {Who Is Using Scikit-Learn? \textemdash{} Scikit-Learn 0.23.1 Documentation},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\C6JUB9Y9\\testimonials.html},
  howpublished = {https://scikit-learn.org/stable/testimonials/testimonials.html}
}

@misc{patel_implementing_2020,
  title = {Implementing a {{Multi Layer Perceptron Neural Network}} in {{Python}}},
  author = {Patel, Vivek},
  year = {2020},
  month = may,
  abstract = {Don't reinvent the wheel, unless you learn something by doing so.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\NIFA257H\\implementing-a-multi-layer-perceptron-neural-network-in-python-b22b5a3bdfa3.html},
  howpublished = {https://towardsdatascience.com/implementing-a-multi-layer-perceptron-neural-network-in-python-b22b5a3bdfa3},
  journal = {Medium},
  language = {en}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research}
}

@misc{shmueli_multi-class_2020,
  title = {Multi-{{Class Metrics Made Simple}}, {{Part II}}: The {{F1}}-Score},
  shorttitle = {Multi-{{Class Metrics Made Simple}}, {{Part II}}},
  author = {Shmueli, Boaz},
  year = {2020},
  month = jun,
  abstract = {How to compute F1-scores (micro, macro, weighted) in the multi-class case, what do mean, and why you shouldn't use them!},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\BHCUU5LM\\multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1.html},
  howpublished = {https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1},
  journal = {Medium},
  language = {en}
}

@misc{stokfiszewski_soft_nodate,
  title = {Soft {{Computing Laboratory}} 1. {{The Delta}} Rule},
  author = {Stokfiszewski, Kamil},
  publisher = {{Institute of Information Technology , Technical University of Lodz}},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\X5G32KCG\\soft_comp_lab_01_DELTA.pdf}
}

@article{swischuk_projection-based_2019,
  title = {Projection-Based Model Reduction: {{Formulations}} for Physics-Based Machine Learning},
  shorttitle = {Projection-Based Model Reduction},
  author = {Swischuk, Renee and Mainini, Laura and Peherstorfer, Benjamin and Willcox, Karen},
  year = {2019},
  month = jan,
  volume = {179},
  pages = {704--717},
  issn = {0045-7930},
  doi = {10.1016/j.compfluid.2018.07.021},
  abstract = {This paper considers the creation of parametric surrogate models for applications in science and engineering where the goal is to predict high-dimensional output quantities of interest, such as pressure, temperature and strain fields. The proposed methodology develops a low-dimensional parametrization of these quantities of interest using the proper orthogonal decomposition (POD), and combines this parametrization with machine learning methods to learn the map between the input parameters and the POD expansion coefficients. The use of particular solutions in the POD expansion provides a way to embed physical constraints, such as boundary conditions and other features of the solution that must be preserved. The relative costs and effectiveness of four different machine learning techniques\textemdash neural networks, multivariate polynomial regression, k-nearest-neighbors and decision trees\textemdash are explored through two engineering examples. The first example considers prediction of the pressure field around an airfoil, while the second considers prediction of the strain field over a damaged composite panel. The case studies demonstrate the importance of embedding physical constraints within learned models, and also highlight the important point that the amount of model training data available in an engineering setting is often much less than it is in other machine learning applications, making it essential to incorporate knowledge from physical models.},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\YPALB4YS\\Swischuk et al. - 2019 - Projection-based model reduction Formulations for.pdf;C\:\\Users\\bhadr\\Zotero\\storage\\P8LCFKSX\\S0045793018304250.html},
  journal = {Computers \& Fluids},
  keywords = {Data-driven reduced models,Model reduction,Physics-based machine learning,Proper orthogonal decomposition,Surrogate models},
  language = {en}
}

@misc{terence_parr_dtreeviz_nodate,
  title = {Dtreeviz},
  author = {{Terence Parr} and {Prince Grover}},
  howpublished = {https://github.com/parrt/dtreeviz}
}

@incollection{witten_appendix_2017,
  title = {Appendix {{B}} - {{The WEKA}} Workbench},
  booktitle = {Data {{Mining}} ({{Fourth Edition}})},
  editor = {Witten, Ian H. and Frank, Eibe and Hall, Mark A. and Pal, Christopher J.},
  year = {2017},
  edition = {Fourth Edition},
  pages = {553--571},
  publisher = {{Morgan Kaufmann}},
  doi = {10.1016/B978-0-12-804291-5.00024-6},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\P4NG6SYG\\Witten_et_al_2016_appendix.pdf},
  isbn = {978-0-12-804291-5}
}

@misc{yadav_support_2018,
  title = {{{SUPPORT VECTOR MACHINES}}({{SVM}})},
  author = {Yadav, Ajay},
  year = {2018},
  month = oct,
  abstract = {Introduction: All you need to know about Support Vector Machines (SVM).},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\WDLPLGQC\\support-vector-machines-svm-c9ef22815589.html},
  howpublished = {https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589},
  journal = {Medium},
  language = {en}
}

@article{zamzam_physics-aware_2019,
  title = {Physics-{{Aware Neural Networks}} for {{Distribution System State Estimation}}},
  author = {Zamzam, Ahmed S. and Sidiropoulos, Nicholas D.},
  year = {2019},
  month = jul,
  abstract = {The distribution system state estimation problem seeks to determine the network state from available measurements. Widely used Gauss-Newton approaches are very sensitive to the initialization and often not suitable for real-time estimation. Learning approaches are very promising for real-time estimation, as they shift the computational burden to an offline training stage. Prior machine learning approaches to power system state estimation have been electrical model-agnostic, in that they did not exploit the topology and physical laws governing the power grid to design the architecture of the learning model. In this paper, we propose a novel learning model that utilizes the structure of the power grid. The proposed neural network architecture reduces the number of coefficients needed to parameterize the mapping from the measurements to the network state by exploiting the separability of the estimation problem. This prevents overfitting and reduces the complexity of the training stage. We also propose a greedy algorithm for phasor measuring units placement that aims at minimizing the complexity of the neural network required for realizing the state estimation mapping. Simulation results show superior performance of the proposed method over the Gauss-Newton approach.},
  archivePrefix = {arXiv},
  eprint = {1903.09669},
  eprinttype = {arxiv},
  file = {C\:\\Users\\bhadr\\Zotero\\storage\\9TW4JLDV\\Zamzam i Sidiropoulos - 2019 - Physics-Aware Neural Networks for Distribution Sys.pdf;C\:\\Users\\bhadr\\Zotero\\storage\\Z7TLLMRU\\1903.html},
  journal = {arXiv:1903.09669 [cs, math]},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}


